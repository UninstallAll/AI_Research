% !TeX program = pdflatex
\documentclass[sigconf,nonacm]{acmart}
\settopmatter{printacmref=false}
\pagestyle{plain}

%-------------
% Packages
%-------------
\usepackage{comment}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{balance}
\usepackage[utf8]{inputenc} % ensure UTF-8 symbols handled safely
\bibliographystyle{ACM-Reference-Format}
%-------------
% Anonymization toggle
%-------------
\newif\ifanonymous
\anonymoustrue % 切换为 \anonymoustrue 以生成匿名版本

% 若开启匿名模式，重写作者信息并隐藏署名
\ifanonymous
  \renewcommand{\shortauthors}{Anonymous}
  \author{Anonymous Author(s)}
\fi

%-------------
% Title & Authors
%-------------
\title{From Code to Camera: The Making and Meaning of Prosomoíosi (Simulation), an AI Documentary Film}

\ifanonymous
  % --- ANONYMOUS VERSION ---
  \author{Anonymous Author(s)}
  \affiliation{%
    \institution{Anonymous Institution}
    \city{Anonymous City}
    \country{Anonymous Country}
  }
  \email{anonymous@email.address}
  \renewcommand{\shortauthors}{Anonymous}

\else

\author{Shuai Liu}
\affiliation{%
  \institution{Academy of Media Arts Cologne (KHM)}
  \city{Cologne}
  \country{Germany}}
\email{shuai.liu@khm.de}

\author{Mar Canet Sola}
\orcid{0000-0001-5986-3239}
\affiliation{%
  \institution{BFM, Tallinn University, Estonia}
  \city{}
  \country{}}
\affiliation{%
  \institution{Academy of Media Art Cologne(KHM), Germany}
  \city{}
  \country{}}
\email{mar.canet@gmail.com}

\renewcommand{\shortauthors}{L. Shuai, M. Canet Sola}
\fi

%-------------
% Abstract
%-------------
\begin{abstract}
% Prosomoíosi (Simulation) is a real-time audiovisual exploration of how successive modelling frameworks overwrite cultural memory. Grounded in media archaeology, the work introduces the concept of \emph{medium alignment}, arguing that ideas must be voiced through media whose operative logic remains visible. A live diffusion pipeline—combining TouchDesigner, StreamDiffusion, and ComfyUI Flux up-scaling—stages the algorithmic politics of selective remembrance and invites audiences to renegotiate authorship at the human–machine frontier.
This paper investigates the audiovisual artwork “Prosomoíosi (Simulation)”, advancing the notion of “medium alignment” to critique how successive simulation technologies overwrite cultural memory. Grounded in media-archaeological thinking, the study argues that concepts must be articulated through media whose operative logic remains visible, thereby transforming viewers from passive spectators into active witnesses of algorithmic decision-making. A live diffusion pipeline—disclosed rather than concealed—functions as both method and message, allowing audiences to see how text prompts, stochastic noise, and performer input co-evolve on screen. By tracing precedents from early interactive installations to recent AI-driven pieces, the paper situates Prosomoíosi within a lineage that questions tool-centric spectacle and re-negotiates authorship at the human–machine frontier. Medium alignment thus emerges as a transferable design heuristic for artists seeking to move beyond technological virtuosity toward works that openly expose, rather than obscure, the politics of their own making.
\end{abstract}

%-------------
% CCS Concepts & Keywords
%-------------
\begin{CCSXML}
% dummy placeholder
\end{CCSXML}
\ccsdesc[500]{Applied computing~Media arts}
\ccsdesc[300]{Computing methodologies~Artificial intelligence}

\keywords{generative AI, medium alignment, real-time diffusion, text-to-video synthesis, StreamDiffusion, TouchDesigner, AI film, media archaeology, fiction documentary}

% inserted to start the document body
\begin{document}

\begin{teaserfigure}
    \centering
    \includegraphics[width=1\linewidth]{pics Prosomoiosi 4pages short paper/band.jpg}
    \caption{Screenshot of Prosomoíosi (Simulation), generated from Cultural Analytics's cover\cite{manovich_cultural_analytics_2020}}
    \label{fig:enter-label}
\end{teaserfigure}

% \begin{teaserfigure}
%     \centering
%     \includegraphics[width=1\linewidth]{screenshot2.jpg}
%     \caption{Screenshot of Prosomoíosi (Simulation)}
%     \label{fig:enter-label}
% \end{teaserfigure}

%-------------
% Title block
%-------------
\maketitle

%-------------
% 1. Introduction
%-------------
\section{Introduction}

Over the past decade, the impact of generative artificial intelligence (AI) on audiovisual media has moved rapidly from research laboratories to the core workflows of the film industry. In 2024 the Academy of Motion Picture Arts and Sciences explicitly allowed films containing AI-generated footage to compete for Oscars—a symbolic step that not only reflects Hollywood's recognition of a technological paradigm shift but also signals a loosening of traditional production standards. In the same year, the Runway AI Film Festival (New York and Los Angeles) and the RAIN Film Fest (Barcelona) were launched, indicating that AI-based image creation is transitioning from peripheral artistic practice to a scaled ecosystem; the inauguration of the first Asian AI Film Festival (HKUST AI Film Festival) in 2025 further confirms the global spread and cultural diversification of this trend.

Technically, breakthroughs in generative models for script writing, storyboarding, image synthesis, and even long-form video generation have opened unprecedented real-time iterative spaces for creators. Text-to-video models such as Runway Gen-2\footnote{\url{http://runwayml.com}}, OpenAI's Sora\footnote{\url{https://openai.com/sora/}}, and Google's Veo\footnote{\url{https://deepmind.google/models/veo/}}, built on large-scale spatio-temporal diffusion frameworks, produce controllable, high-definition, and narratively coherent output; meanwhile, text-to-image models like Stable Diffusion\footnote{https://huggingface.co/CompVis/stable-diffusion-v1-4}, Flux\footnote{\url{https://huggingface.co/black-forest-labs/FLUX.1-dev}}, and Midjourney\footnote{\url{https://www.midjourney.com/}}, together with large language models (LLMs) for script development and world-building, are reshaping the traditional pre-production workflow. Researchers note that AI tools not only increase production efficiency but also introduce a "machine vision" cultural grammar that co-evolves with audience perception.

Yet this technological surge exposes a largely overlooked contradiction: when artists embrace a new medium while retaining an old conceptual framework—or conversely, use new concepts to drive an old medium—a misalignment arises between medium and idea. The consequences are typically:  
\begin{enumerate}
    \item The new technology is treated merely as an "accelerator," unable to participate substantively in the work's intent;
    \item The concept is forced to yield to the tool's default paradigm, resulting in demonstrative or techno-virtuosic superficiality;
    \item Audiences perceive chiefly a "technological spectacle" rather than the work's thematic core, thereby weakening attention to the artistic statement itself.
\end{enumerate}

% This article addresses the phenomenon of "medium/concept misalignment" by taking the AI video work Prosomoíosi (Simulation) as its central case study. The piece adopts "simulation" as its entry point and, through a series of investigations into the technical, historical, and cultural semantics of simulation, gradually formulates and explicates the notion of "medium alignment": the deliberate re-alignment of a work's conceptual agenda with the operational logic of the medium employed, so that the technical procedure becomes a perceptible part of expression rather than a concealed backstage. The work's technical architecture—in which StreamDiffusion\footnote{https://github.com/cumulo-autumn/StreamDiffusion} is invoked inside TouchDesigner\footnote{https://derivative.ca/} for real-time image generation, augmented by dynamic parameter curves and ComfyUI Flux-based tiled up-scaling—embodies this idea: the diffusion process is rendered visible and incorporated into the narrative, enabling audiences to perceive directly how algorithms shape the image. The paper contends that only under the premise of medium alignment can generative moving images transcend tool-oriented and techno-virtuosic tendencies to become a dynamic arena for renegotiating memory, history, and subjectivity.
Generative AI is rapidly reshaping film and art production, yet conceptual thinking has not kept pace, widening the gap between medium and idea. Using the video work Prosomoíosi (Simulation) as a case study, we advance a speculative notion of “medium alignment”: aligning a project’s theme with the operative logic of its medium from the outset so that the algorithmic pipeline becomes a visible narrative layer rather than backstage infrastructure. This proposal echoes Baudrillard’s model-first simulation logic \cite{baudrillard_simulacra_1994}, Bolter and Grusin’s remediation paradigm \cite{bolter_remediation_1999}, and Manovich’s analysis of algorithmic storytelling \cite{manovich_language_2001}.
Prosomoíosi literalises “simulation”: real-time images are produced in TouchDesigner\footnote{\url{https://derivative.ca/}} via StreamDiffusion\footnote{\url{https://github.com/cumulo-autumn/StreamDiffusion}}, driven by key-framed parameters, then tiled and up-scaled to 4K with ComfyUI and Flux model. By exposing the diffusion process on-screen, the work lets viewers watch algorithms overwrite the image itself. We argue that such medium alignment enables generative video to transcend tool fetishism and techno-virtuosity, turning it into a dynamic arena where memory, history, and subjectivity can be renegotiated.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pics Prosomoiosi 4pages short paper/compare.jpg}
    \caption{Image quality comparison, left:previous work; right: this work}
    \label{fig:enter-label}
\end{figure*}
\section{Reference Artworks}

To contextualize our research, this section examines a selection of artworks that trace the evolution from early interactive systems to contemporary AI-driven critiques of perception. A foundational example is A\textendash \textit{Volve} (1994) by Christa Sommerer and Laurent Mignonneau, a pioneering installation where visitors' drawings are instantiated as virtual creatures \cite{AVolve}. The work foregrounds themes of artificial life and embodied interaction, positioning the human as a direct creator within a digital ecosystem. Decades later, this focus on perception and digital life has been radically reconfigured by deep learning. Rather than granting users direct creative agency, many contemporary artists use AI to interrogate the very process of seeing. For instance, Memo Akten's \textit{Learning to See} (2017) employs real-time neural networks to reveal how a model reconstructs reality through the biased filter of its training data, explicitly questioning the relationship between perception and truth \cite{LearningToSeeWeb,LearningToSeePaper}. A related, though aesthetically distinct, exploration is Scott Eaton's \textit{Entangled II} (2019), which uses a bespoke neural network to process abstract footage into morphing, quasi-human figures, examining the aesthetics of machine perception and the human tendency for pareidolia \cite{EntangledII}. Taking this critique of data's influence a step further, \textit{POSTcard Landscapes from Lanzarote} (2022) by Varvara \& Mar leverages a StyleGAN2 model trained on distinct datasets to generate two opposing visual realities of the same location: the tourist's view and the local's view \cite{Guljajeva2022}. By juxtaposing these AI-generated "gazes," the work offers a powerful commentary on how curated data not only reflects but actively shapes cultural identity and memory, echoing John Urry's concept of the "tourist gaze" \cite{Urry2002}. The author of the present paper likewise extended this trajectory with a 2024 artwork that employs real-time generative techniques to explore a symbiotic relationship between AI and human creators, the quality of generated videos comparison is below in Figure 2. Together, these works illustrate a significant shift from celebrating direct interaction with artificial life to a critical investigation of how AI models, and the data they are fed, mediate and construct our reality.


\begin{comment}
\begin{description}
  \item[\textbf{A\textendash Volve} (1994) by Christa Sommerer\, \& Laurent Mignonneau.] An interactive installation where visitors draw two-dimensional life-forms that are immediately instantiated as three-dimensional virtual creatures swimming inside a water basin. These entities compete, evolve, and respond to human touch, foregrounding artificial life and embodied interaction \cite{AVolve}.

  \item[\textbf{Entangled II} (2019) by Scott Eaton.] A 4K vertical video that processes high-speed footage of paint dispersing in water through a bespoke neural network ("BODIES") to produce continuously morphing, quasi-human figures. The work examines pareidolia and the aesthetics of machine perception \cite{EntangledII}.

  \item[\textbf{Learning to See} (2017 ) by Memo Akten.] An ongoing series employing real-time pix2pix networks to reveal how neural models reconstruct reality through the filter of prior data, thereby interrogating perception, bias, and truth. A detailed technical and conceptual discussion appears in Akten's SIGGRAPH 2019 paper \cite{LearningToSeeWeb,LearningToSeePaper}.

  \item[\textbf{POSTcard Landscapes from Lanzarote} (2022 ) by Varvara \& Mar.] The artwork "POSTcard Landscapes from Lanzarote" by Varvara Guljajeva and Mar Canet is an AI-generated audiovisual piece exploring differing perspectives of tourists and locals regarding a specific place \cite{Guljajeva2022}. The artwork presents two distinct videos side by side, examining the relationship between tourism, memory, and local identity through two opposing synthetic realities created by AI using collected data. The visual material was generated with the StyleGAN2 algorithm, trained on found footage datasets from Flickr depicting Lanzarote, a tourist hotspot in the Canary Islands. The artists produced two separate videos representing touristic and local views of the island. This AI-driven synthesis highlights how tourist images shape and sometimes overshadow the genuine cultural identity of places, reflecting John Urry’s concept of the "tourist gaze,"\cite{Urry2002} where photographic reproduction eclipses authentic experiences. The latent-space interpolation technique transforms static image data into dynamic, fluid videos complemented by soundtracks from two local sound artists, enriching the critical and atmospheric dimensions of the piece. Ultimately, the project emphasizes the crucial role datasets play in AI, demonstrating how memory construction can be represented through synthetic processes powered by artificial intelligence.

\end{description}
\end{comment}
\section{Case Study : Prosomoiosi (Simulation)}

\subsection{Artwork Description}

Prosomoíosi (Simulation) is a video work created through a real-time generative pipeline that, through a media-archaeological lens, advances the notion of "medium alignment" to probe how successive technological regimes continually overwrite memory and identity. The work pursues two mutually reinforcing aims: (1) to unmask, through a stratified narrative of simulation logics, the quiet manner in which each new medium overprints cultural remembrance; and (2) to disclose the algorithmic selectivity and symbolic authority of AI-driven image synthesis by staging a reflexive demonstration built on a TouchDesigner–StreamDiffusion pipeline featuring real-time multiprompt editing and ComfyUI up-scaling with the flux-dev f16 model. By allowing text prompts, stochastic noise, and live performer input to co-evolve on the exhibition floor, the piece shifts image production from depiction to ontological negotiation, inviting audiences to rethink the future interplay of human creativity and machine simulation.

Simulation here is not mere copying or representation; rather, it is a power-laden rewriting produced through the interplay of technology, archives, and algorithms. Wolfgang Ernst notes that archival temporality shapes memory even as it quietly edits the past \citep{Ernst_Archive}; Walter Benjamin anticipated the aura's erosion under mechanical reproduction \citep{Benjamin_WorkOfArt}, and Jean Baudrillard later warned of simulacra supplanting reality \cite{Baudrillard_Simulacra}. Charting a trajectory "from clay to code"—from sand tables and armillary spheres to deep learning and GANs—modeling technologies have evolved from cognitive tools to arbiters of truth: large networks such as GPT-4 and ESM-2 autonomously conjure worlds from latent space and even guide human decision-making, reshaping the human–reality relation. Within this landscape, video games have become the most pervasive simulation medium; by breaching the "fourth wall," they place players in a hybrid third space where procedural rhetoric lets symbolic capital and ideology permeate interaction, extending McLuhan's dictum that "the medium is the message" \cite{McLuhan_UnderstandingMedia} alongside Bourdieu's analysis of symbolic power \cite{Bourdieu_SymbolicPower}. Educational and political simulations—SimCity, PeaceMaker—as well as Lorenz-style chaotic systems further attest to simulation's profound influence on behaviour and cognition. Faced with the medium mutations of algorithmic culture, art must enact medium alignment, expanding AI alignment into a perceptual-symbolic calibration at the level of the medium itself; situated between Stiegler's "third memory" \cite{Stiegler_TechnicsTime1} and Yuk Hui's "cosmic technics," \cite{Hui_Cosmotechnics} this posture seeks to cultivate a future symbiosis among humans, machines, and media.



\subsection{Production Workflow}
The real-time generation pipeline of this project consists of four stages: 'material preparation $\rightarrow$ diffusion generation $\rightarrow$ high-resolution upscaling $\rightarrow$ post-production,' with each stage optimized for live performance.

{Material Preparation.}
Source footage includes public-domain vintage films and live gameplay recordings, uniformly transcoded to 24 fps H.264 in Adobe Premiere Pro. The H.264 codec utilizes advanced video compression techniques including motion estimation, transform coding, and entropy coding to achieve optimal file sizes while maintaining visual quality\footnote{Wiegand, T., et al. (2003). Overview of the H. 264/AVC video coding standard. IEEE Transactions on circuits and systems for video technology, 13(7), 560-576.}. To optimize diffusion processing on RTX 4090 hardware, the footage is slowed to 4-14 fps (0.2×-0.5× speed) and streamed through NDI (Network Device Interface)\footnote{NewTek NDI SDK Documentation: \url{https://www.ndi.tv/sdk/}} to TouchDesigner for real-time generation.

The frame rate reduction strategy is crucial for maintaining real-time performance as diffusion models have significant computational overhead. Each frame requires multiple denoising steps through the U-Net architecture\footnote{Ronneberger, O., Fischer, P., \& Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241).}, making higher frame rates computationally prohibitive on current hardware. The NDI protocol enables low-latency, high-quality video streaming over standard Ethernet networks, providing crucial timing precision for real-time generative workflows\footnote{Perkins, C. (2003). RTP: Audio and Video for the Internet. Addison-Wesley Professional.}.

{Real-Time Diffusion Generation (TouchDesigner + StreamDiffusion plug-in).}
Hardware platform: AMD Ryzen 9 7950X / NVIDIA RTX 4090 (24 GB VRAM).
In TouchDesigner, the StreamDiffusion node developed by dotsimulate\footnote{\url{https://dotsimulate.com/}} is loaded. StreamDiffusion is an optimization framework that enables near real-time diffusion model inference by implementing stream batching and residual classifier-free guidance\footnote{Yamamoto, A., et al. (2023). StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation. arXiv preprint arXiv:2312.12491.}. After configuring the project's dependencies, every video frame undergoes image-to-image (img2img) diffusion using denoising diffusion probabilistic models (DDPMs)\footnote{Ho, J., Jain, A., \& Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.}. 

Models such as Stable Diffusion XL\footnote{Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv preprint arXiv:2307.01952.}, Stable Diffusion 1.5\footnote{Rombach, R., et al. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).}, and their LoRAs (Low-Rank Adaptations)\footnote{Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.} are switched dynamically to achieve desired artistic effects. The latent diffusion architecture operates in a compressed latent space using a variational autoencoder (VAE), significantly reducing computational requirements compared to pixel-space diffusion\footnote{Kingma, D. P., \& Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.}.

StreamDiffusion parameters—Prompt, CFG Scale (Classifier-Free Guidance)\footnote{Ho, J., \& Salimans, T. (2022). Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598.}, Steps, Denoise, etc.—are bound to keyframe curves via the Animation COMP curve editor in TouchDesigner, allowing precise control over all generation parameters. Performers can pre-program or live-edit these curves to sculpt narrative rhythm, achieving smoother and more coherent results through temporal consistency optimization.

The workflow also integrates ControlNet's HED (Holistically Nested Edge Detection)\footnote{Xie, S., \& Tu, Z. (2015). Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision (pp. 1395-1403).} branch within TouchDesigner; by parameterizing the edge-strength weight, the diffusion process is further constrained to converge on scene geometry and spatial alignment. ControlNet enables fine-grained control over the generation process by conditioning the diffusion model on additional input modalities such as edge maps, depth maps, or pose estimations\footnote{Zhang, L., \& Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543.}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{pics Prosomoiosi 4pages short paper/TD detail.png}
    \caption{Screen capture of the TouchDesigner workflow: red – Animation COMP–related modules; green – StreamDiffusion node; yellow – ControlNet}
    \label{fig:enter-label}
\end{figure}


% In this Flux workflow for high-definition image generation, preprocessing and region-wise reconstruction form the core foundation. The overall design aims to ensure generation quality, adapt to different VRAM environments, enhance large-image handling, and improve detail expression.
% First, the image is imported via a LoadImage node and uniformly resized to a standard size (e.g., 1024~$\times$~1024) with an ImageResizeKJ node, ensuring compatibility with downstream modules while controlling interpolation. Next, an UpscaleModelLoader node loads a super-resolution model (e.g., "4x NMKD-Siax 200k"), and the ImageUpscaleWithModel node performs a true 4$\times$ enlargement. This step not only increases resolution but also lays a high-dimensional foundation for subsequent generation and refinement, thereby enhancing overall texture and edge sharpness.
{High-Resolution Upscaling (ComfyUI Flux Workflow).}
The Flux pipeline first loads each image, resizes it to 1024 × 1024 for VRAM-adaptive processing, then applies a 4× NMKD-Siax super-resolution pass; this tile-based routine balances memory, accommodates large canvases, and boosts texture and edge detail for later stages. Flux represents a new generation of diffusion models that utilizes flow matching instead of traditional denoising approaches\footnote{Liu, X., et al. (2022). Flow matching for generative modeling. arXiv preprint arXiv:2210.02747.}, enabling more efficient training and inference while maintaining high-quality output.

The NMKD-Siax upscaling model is based on Real-ESRGAN architecture\footnote{Wang, X., et al. (2021). Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1905-1914).}, which employs a generative adversarial network (GAN) specifically designed for real-world image super-resolution. The model utilizes Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN)\footnote{Wang, X., et al. (2018). ESRGAN: Enhanced super-resolution generative adversarial networks. In Proceedings of the European conference on computer vision (ECCV) (pp. 63-79).} with improved perceptual loss functions and residual-in-residual dense blocks for better texture preservation.

To reduce VRAM load at high resolutions, we use the Tile-to-Patch (TTP) toolset\footnote{\url{https://github.com/TTPlanetPig/Comfyui_TTP_Toolset}}. This approach implements a sliding window technique with overlap compensation to prevent artifacts at tile boundaries\footnote{Zhang, K., et al. (2019). Aim 2019 challenge on constrained super-resolution: Methods and results. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) (pp. 3565-3574).}. TTP automatically sizes tiles (\texttt{TTP\_\allowbreak Tile\_\allowbreak image\_\allowbreak size}), splits the image into batches (\texttt{TTP\_Image\_Tile\_Batch}) for parallel latent-space processing, and then reassembles them (\texttt{TTP\_Image\_Assy}), delivering a high-detail image while keeping memory usage in check. The tiling strategy ensures global consistency by maintaining overlapping regions and applying seamless blending algorithms during reconstruction\footnote{Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{pics Prosomoiosi 4pages short paper/TTP.png}
%     \caption{Nodes related to TTP functions are marked with red boxes}
%     \label{fig:enter-label}
% \end{figure}

\paragraph{Output and Post-Production.}
The 4K PNG sequence generated by the Flux pipeline is brought into After Effects at 24fps for grading, depth-of-field, and lighting (Lumetri Color, Camera Lens Blur). The PNG format ensures lossless quality preservation throughout the post-production pipeline\footnote{Boutell, T. (1997). PNG (Portable Network Graphics) specification version 1.0. RFC 2083.}. The graded master is output to DCI 4K (4096 × 2160, 24 fps, Rec. 709 Gamma 2.4) following digital cinema standards\footnote{Digital Cinema Initiative (2012). Digital Cinema System Specification Version 1.2.}. 

Color grading utilizes the Rec. 709 color space, which provides standardized color reproduction for high-definition television and digital cinema applications\footnote{ITU-R Recommendation BT.709-6 (2015). Parameter values for the HDTV standards for production and international programme exchange.}. The Lumetri Color panel implements industry-standard color correction algorithms including lift-gamma-gain adjustments and HSL (Hue, Saturation, Lightness) curves for precise color manipulation\footnote{Fairchild, M. D. (2013). Color appearance models. John Wiley \& Sons.}. Camera Lens Blur applies depth-of-field effects using circle of confusion calculations to simulate authentic optical characteristics\footnote{Ray, S. F. (2002). Applied photographic optics: lenses and optical systems for photography, film, video, electronic and digital imaging. Focal Press.}.

The final output is further polished in Topaz Video AI\footnote{\url{https://www.topazlabs.com/}} using Chronos 2-4× frame interpolation. Chronos employs deep learning-based motion estimation and frame synthesis algorithms to generate intermediate frames, effectively increasing temporal resolution while maintaining motion coherence\footnote{Niklaus, S., Mai, L., \& Liu, F. (2017). Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE International Conference on Computer Vision (pp. 261-270).}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{pics Prosomoiosi 4pages short paper/ComfyUI1.png}
    \caption{Overview of comfyUI upscaling workflow}
    \label{fig:enter-label}
\end{figure}

\subsection{Presentation format}
The piece is presented in two formats:
(1) A 4K DCP version (24/50 fps, REC.709, 5.1 surround sound) designed for theatrical or festival screenings;
(2) A gallery edition, looped from the same final master, projected in 4K with four-channel audio. Both formats maintain identical color grading and dynamic range, ensuring consistent presentation across black-box theaters and white-cube gallery settings.

%-------------
% 4. Conclusion
%-------------

\section{Discussion}


% Second, the necessity—and future potential—of real-time image generation warrants closer scrutiny. Real-time pipelines push the act of “capture” into an ontological negotiation unfolding in the present: parameters, textual prompts and performer interventions co-evolve, exposing the algorithmic contingencies of every frame. Although such immediacy is limited by current hardware throughput and model latency, these very constraints constitute a visible grammar that can be critically mobilised. By contrast, non-real-time generation resembles the analogue darkroom: the temporal distance allows for meticulous post-production, compositing and authorial revision, yet simultaneously obscures the causal chain between algorithm and image. Both approaches offer legitimate creative affordances; the key, again, is to return to first principles and decide whether a work’s conceptual agenda is better served by the transparency and performativity of real-time synthesis or by the reflective plasticity of deferred rendering.
First, The potential of real-time image synthesis should be evaluated within a human–machine collaboration frame. As Mar Canet Sola notes, every “automatic” frame still requires the artist to architect the pipeline, select and tune models, and make iterative aesthetic and ethical calls \cite{guljajeva_canet_sola_artist_2024}. AI is not a one-click oracle; it resembles developer fluid or pigment—only when the creator controls prompts, weights, and workflow does it speak a personal visual language. Just as photographers rely on framing, metering, and dark-room work, real-time diffusion relies on human calibration of rhythm, light, and meaning. Therefore, before choosing live over offline generation, we must ask whether a project truly benefits from the transparency and improvisation of on-stage human-AI performance, rather than assuming the tool can replace authorship.

% First, Elon Musk's insistence on "reasoning from first principles rather than by analogy"—i.e., reducing a problem to its most irreducible truths before rebuilding a solution \cite{Brier2014}—illuminates a blind spot in contemporary media art. Too often, artists choose a medium by analogy: video because predecessors used video, or AI because it is trending. Such analogical thinking risks misalignment between medium and concept, resulting in works whose technological veneer masks conceptual inertia. By contrast, a first-principles approach would begin with the core question of what a piece wishes to express and only then select (or invent) the medium whose operational logic best embodies that inquiry. Medium alignment, in this sense, is not an aesthetic luxury but a methodological necessity; if the foundational choice of medium is ill-considered, subsequent formal refinements become largely futile.
Elon Musk’s first-principles dictum—strip a problem to its basics, then build up \cite{Brier2014}—shows what media art often gets wrong: choosing video or AI for fashion, then grafting on meaning. Start with the idea, then pick or invent the medium that enacts it; without that alignment, later polish is pointless.

% The creative community has already begun releasing lightweight open-source toolkits for live diffusion, such as Vadim Epstein’s SDfu framework\footnote{\url{GitHub repository: https://github.com/eps696/SDfu}}. Epstein showcases the tool’s use in live performance and video production on his website\cite{epstein_wesual_2025} and in the experimental short film The Poem\cite{epstein_poem_2025}, reminding us that real-time synthesis is not only a technical issue but also a gateway to new viewing relations and modes of authorship.
Lightweight open-source toolkits such as Vadim Epstein’s SDfu\footnote{\url{https://github.com/eps696/SDfu}} let artists shape real-time diffusion instead of merely triggering it. Epstein demonstrates this in live sets and in the short film The Poem \cite{epstein_wesual_2025,epstein_poem_2025}, showing that “one-click” AI is a myth: prompt design, parameter tuning, and aesthetic judgment remain firmly human tasks. Real-time synthesis thus becomes less a technical shortcut than a new stage for co-authoring perception and authorship.

\section{Conclusion}
This paper has argued that contemporary generative AI practice requires more than technical novelty; it demands a principled reconciliation between medium and concept. Building on Musk's call for first-principles reasoning, we reframed medium choice as a foundational decision rather than a cosmetic afterthought. The notion of medium alignment articulated here positions the operative logic of a medium—its algorithms, temporalities, and interfaces—as an integral layer of meaning production.
Through the case study of Prosomoíosi (Simulation) we demonstrated how a TouchDesigner–StreamDiffusion pipeline, augmented by live parameter editing and Flux up-scaling, can turn real-time image synthesis into both method and message. The work exposes the algorithmic contingencies of diffusion while inviting audiences to negotiate authorship in the very moment of visual emergence. Our comparative discussion showed that real-time and non-real-time pipelines offer distinct affordances—immediacy and performativity versus reflective plasticity—and that the critical task is to select the approach that most faithfully embodies a project's conceptual agenda.
Medium alignment thus emerges as a transferable design heuristic for artists and researchers navigating an era in which technical frameworks evolve faster than critical discourse. By treating first-principles reasoning and real-time generativity not as ends in themselves but as instruments of conceptual clarity, future practices can move beyond tool-centred spectacle toward works that disclose, rather than disguise, the politics of their own making.

%-------------
% Biographies
%-------------
%\section*{Biographies}
%\textbf{Liu Shuai} is a Chinese digital media artist based in Cologne and Beijing. He earned a BFA in Digital Media Art from Guangzhou Academy of Fine Arts and is currently pursuing an MFA at the Academy of Media Arts Cologne. Since 2024 he has served as a research fellow at the Digital Humanities Research Center of Renmin University of China and as an advisor to Jinan University's Centre for AI and New Media Art. Liu's practice spans algorithmic image making, interactive games, and network installations. His works have received the Goethe-Institut "AIsolation" AI Short-Film Award, the Jury Recommendation at the UK Lift-Off Network's "First Time" Filmmakers Session, Official Selection in the Experimental Section of the Beijing International Short Film Festival, the Gold Prize for Digital Imaging at the China University AI Art Season, the Art Experiment Prize at Poland's On Art Festival, and a Jury Special Mention at the Student World Impact Film Festival (USA). Earlier in his career he worked at the Guangdong Museum of Art and Guangdong Times Museum, gaining curatorial and institutional experience. In 2024 he co-founded the experimental game magazine LUDUS, which collaborates with the Academy of Media Arts Cologne and the University of Applied Arts Vienna to foster interdisciplinary exchange on AI art and game studies.

%\textbf{Mar Canet Sola} is a Spanish media artist, designer, and researcher, best known as the co-founder of the artist duo Varvara \& Mar (est. 2009). He is currently a PhD candidate and research fellow in the CUDAN Research Group at the Baltic Film, Media and Arts School, Tallinn University. Canet holds an MSc in Interface Cultures from the University of Art and Design Linz, two degrees in art and design from ESDI Barcelona, and a BSc (Hons) in Computer Game Development from the University of Central Lancashire, UK. His practice investigates generative systems, artificial intelligence, and audience participation through kinetic, data-driven installations. Works created with Varvara Guljajeva have been shown internationally at venues including the Museum of Arts and Design (New York), FACT (Liverpool), Arts Santa Mònica (Barcelona), the Barbican Centre and the Victoria \& Albert Museum (London), Onassis Cultural Centre (Athens), the Ars Electronica Center (Linz), and ZKM | Karlsruhe. Their research has been published in proceedings of SIGGRAPH, SIGGRAPH Asia, IEEE VISAP, ACM TEI, EvoMUSART, ACM Creativity \& Cognition, ACM Multimedia, xCoAx, ISEA, among others.

\section*{Code availability and demo}
The complete ComfyUI–TouchDesigner workflow used in this case study is openly available on GitHub, allowing readers to reproduce and extend our results: \url{https://github.com/UninstallAll/MAAPIIcursor} (see the \texttt{workflow/} folder).
%-------------
% Acknowledgements
%-------------


\ifanonymous
  \section*{Acknowledgements}
  Acknowledgments have been removed for anonymous review and will be restored in the camera-ready version.
\else
    \section*{Acknowledgements}
    The authors thank Shuree Sarantuya and Tianyi Hu (Academy of Media Arts Cologne) for their invaluable assistance—particularly Hu's outstanding contributions to prompt design, text generation, and other production support—as well as the Academy of Media Arts Cologne for overall technical support.
\fi
%-------------
% References
%-------------

\clearpage 
\nobalance 
\bibliographystyle{ACM-Reference-Format}
\bibliography{references} 
\end{document} 